# JPSFT

This repository contains the code for training Llama-3.2-11B-Vision-Instruct-JPSFT. Supervised fine-tuning is performed on a generative pre-trained transformer on a select ~150,000 query-response pairs from a corpus of diverse, anonymized, scraped chat data. This project focuses on standard SFT as well as instruction-tuning for GPT-based architectures to generate coherent and context-aware responses in multi-speaker conversations in Japanese.
