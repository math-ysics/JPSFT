# JPSFT

This repository contains the code for training Llama-JPSFT-11B. Supervised fine-tuning is performed on a generative pre-trained transformer on a select ~150,000 query-response pairs from a diverse corpus of anonymized, scraped chat data, with a priority on casual conversation. This project focuses on standard SFT as well as instruction-tuning for GPT-based architectures to generate significantly improved coherent and context-aware responses in multi-speaker conversations in Japanese.
