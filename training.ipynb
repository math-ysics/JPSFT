{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giM74oK1rRIH",
        "outputId": "0ee25c76-84e2-498b-f90a-d500aca4c574"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please check your GPU setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap_fvMBsQHJc",
        "outputId": "5e6e2c41-2e5d-4efc-b20d-550bcf23c710"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "\n",
        "DATASET_PATH = \"/content/drive/My Drive/alpaca_final.json\"\n",
        "\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5mEBJomyS9O",
        "outputId": "80f371a7-eda3-439e-fbdd-70f5a0c2416d"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS0Qk5OR0i4Q",
        "outputId": "b69ae6cd-d7ed-483d-a693-1ad23f5061af"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "  dataset=\"identity,alpaca_en_demo\",\n",
        "  template=\"llama3\",\n",
        "  finetuning_type=\"lora\",\n",
        "  lora_target=\"all\",\n",
        "  lora_rank=64,\n",
        "  output_dir=\"llama3_lora\",\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=1,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=10,\n",
        "  warmup_ratio=0.1,\n",
        "  save_steps=1000,\n",
        "  learning_rate=5e-5,\n",
        "  num_train_epochs=3.0,\n",
        "  max_samples=float('inf'),\n",
        "  max_grad_norm=1.0,\n",
        "  loraplus_lr_ratio=16.0,\n",
        "  fp16=True, # mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMojogHbaOZF",
        "outputId": "daf542d8-8952-4339-bcec-9174891e0144"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "  adapter_name_or_path=\"llama3_lora\",\n",
        "  template=\"llama3\",\n",
        "  finetuning_type=\"lora\",\n",
        "  export_dir=\"llama3_lora_merged\",\n",
        "  export_size=2,\n",
        "  export_device=\"cpu\",\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O40_yeL80oGI",
        "outputId": "b5988492-e976-4975-b944-e1614ae02e56"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaMA-Factory/llama3_lora_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoWUMQ5j0o0O",
        "outputId": "567ef4ed-efb6-468c-82a0-2b3859e1cd2c"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NxQccrK7210j"
      },
      "outputs": [],
      "source": [
        "!cp * /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
